{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a42f64e-0164-4028-b522-bf53114cfbf8",
   "metadata": {},
   "source": [
    "### CaseHOLD Llama labeling\n",
    "\n",
    "In this notebook, we run a labeling procedure on the test set of the CaseHOLD dataset. The dataset uses a \"citing prompt\", and contains five multiple choice answer options. We ask the LLM to read over the prompt and options, then select the best one.\n",
    "\n",
    "We test three different variants of the Llama LLMs. We record the responses for each model for further analysis elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f474bd-1883-4f92-bbbb-0be96c39144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "client = client(service_name='bedrock-runtime',\n",
    "                      config=config, region_name=\"us-east-1\")\n",
    "\n",
    "ds = load_dataset(\"casehold/casehold\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23205ca0-4420-4e4b-94c9-a818d97e58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(r):\n",
    "    return \"\"\"\n",
    "    Task: Legal Holding Identification\n",
    "    \n",
    "    Context: You are analyzing a legal text to identify the most appropriate legal holding. A legal holding is the court's determination of a matter of law based on the facts of a particular case.\n",
    "    \n",
    "    Input Text: \"{citing_prompt}\"\n",
    "\n",
    "    Question: Based on the legal context above, which of the following holdings best completes the text where the <HOLDING> tag appears? Consider:\n",
    "    - The specific legal issue being discussed\n",
    "    - The logical flow of the legal argument \n",
    "    - The precedential value implied by the context\n",
    "\n",
    "    Options:\n",
    "    A: {holding_0}\n",
    "    B: {holding_1}\n",
    "    C: {holding_2}\n",
    "    D: {holding_3}\n",
    "    E: {holding_4}\n",
    "\n",
    "    Instructions:\n",
    "    1. Analyze the context and legal reasoning in the input text\n",
    "    2. Consider how each option would fit within the legal argument\n",
    "    3. Evaluate which option best maintains the logical flow. Explain your reasoning first, formatted like this <reasoning> reasoning </reasoning>\n",
    "    4. Provide your final answer in the format: ANSWER: X (where X is A, B, C, D, or E)\"\"\".format(\n",
    "        citing_prompt = r['citing_prompt'], holding_0 = r['holding_0'], holding_1 = r['holding_1'], \n",
    "        holding_2 = r['holding_2'], holding_3 = r['holding_3'], holding_4 = r['holding_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc01b6-46a5-4042-9211-67599002349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds['test'].to_pandas()\n",
    "test['label'] = test['label'].astype(float)\n",
    "test['final_prompt'] = test.apply(create_prompt, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9f859-825f-46a3-9519-415f292f5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7a192-78c9-46a4-a0d9-d6b57741eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['response_llama_3b'] = ''\n",
    "test['response_llama_11b'] = ''\n",
    "test['response_llama_90b'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2bd66-9575-496d-b3ab-9218795b591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama32_3b = ChatBedrockConverse(model=\"us.meta.llama3-2-3b-instruct-v1:0\", region_name=\"us-east-1\", temperature = 0, client = client)\n",
    "llm_llama32_11b = ChatBedrockConverse(model=\"us.meta.llama3-2-11b-instruct-v1:0\", region_name=\"us-east-1\", temperature = 0, client = client)\n",
    "llm_llama32_90b = ChatBedrockConverse(model=\"us.meta.llama3-2-90b-instruct-v1:0\", region_name=\"us-east-1\", temperature = 0, client = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d886a-0524-4c74-b917-dc7cabbfee75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, test.shape[0]):\n",
    "    \n",
    "    print(i)\n",
    "    row = test.loc[i]\n",
    "    messages = [\n",
    "        (\"user\", row['final_prompt'])\n",
    "    ]\n",
    "\n",
    "    e = llm_llama32_3b.invoke(messages)\n",
    "    test.loc[i, 'response_llama_3b'] = e.content\n",
    "\n",
    "    f = llm_llama32_11b.invoke(messages)\n",
    "    test.loc[i, 'response_llama_11b'] = f.content\n",
    "\n",
    "    g = llm_llama32_90b.invoke(messages)\n",
    "    test.loc[i, 'response_llama_90b'] = g.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aeba44-bc69-4d26-b5ec-3d7ff8350bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99921c0f-b301-4bb1-9d99-f6902bea2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('casehold_test_llama_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23027e-0e41-49e9-999a-94da5d6caa1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scdb",
   "language": "python",
   "name": "scdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
